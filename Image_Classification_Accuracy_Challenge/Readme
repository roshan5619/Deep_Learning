# ğŸ§  CNN-Based Image Classification on MNIST & STL-10

This project demonstrates the use of **Convolutional Neural Networks (CNNs)** to classify images from two popular datasets: **MNIST** (handwritten digits) and **STL-10** (natural scenes). Both datasets contain **10 distinct object classes**, and our goal is to build deep learning models using **Keras or PyTorch** to accurately classify them.

---

## ğŸ¯ Objective

- Train deep CNNs to classify:
  - ğŸ–Šï¸ **MNIST**: 28x28 grayscale images of handwritten digits.
  - ğŸŒ„ **STL-10**: 96x96 RGB images from 10 object classes.

- Achieve high accuracy on training and test datasets.
- Provide visual insights into the learning process and model predictions.

---

## ğŸ—ƒï¸ Dataset Overview

### ğŸ“¦ MNIST
- 60,000 training images
- 10,000 test images
- Classes: Digits (0â€“9)
- Format: 28x28 grayscale

### ğŸŒ„ STL-10
- 5,000 training images (500/class)
- 8,000 test images (800/class)
- Classes: airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck
- Format: 96x96 RGB

---

## ğŸ§  Model Architecture (Example)

A typical CNN architecture used for both datasets:

```text
Conv2D â†’ ReLU â†’ MaxPool2D
Conv2D â†’ ReLU â†’ MaxPool2D
Flatten â†’ Dense â†’ Dropout â†’ Dense (Softmax)

More complex architectures (e.g., ResNet, VGG) may be used for STL-10 to improve performance.
ğŸ” Data Preprocessing

    Normalize pixel values to [0, 1] for both datasets.

    One-hot encode labels (if using Keras).

    Resize STL-10 images and apply data augmentation (random crop, horizontal flip).

ğŸ§ª Accuracy & Results
Dataset	Model	Train Accuracy	Test Accuracy
MNIST	CNN	~99%	~98.3%
STL-10	CNN	~90%	~80%

    Accuracy may vary slightly based on architecture, training time, and regularization.

ğŸ“Š Visualizations
1. Sample Predictions

MNIST:

STL-10:

2. Accuracy and Loss Curves

MNIST Accuracy:

STL-10 Accuracy:

ğŸ“ Project Structure

ğŸ“¦ CNN_Image_Classification
â”œâ”€â”€ mnist_cnn.py              # CNN training for MNIST
â”œâ”€â”€ stl10_cnn.py              # CNN training for STL-10
â”œâ”€â”€ utils.py                  # Helper functions
â”œâ”€â”€ visuals/                  # Accuracy plots and predictions
â”‚   â”œâ”€â”€ sample_mnist_preds.png
â”‚   â”œâ”€â”€ mnist_accuracy.png
â”‚   â”œâ”€â”€ stl10_accuracy.png
â”œâ”€â”€ README.md                 # Project documentation

ğŸš€ Getting Started
ğŸ“¦ Prerequisites

Install dependencies:

pip install torch torchvision matplotlib numpy

ğŸƒ Run the Code

To train and test on MNIST:

python mnist_cnn.py

To train and test on STL-10:

python stl10_cnn.py

Output graphs and predictions will be saved in the visuals/ folder.
ğŸ”¬ Experiments Performed

    âœ… Compared activation functions: ReLU vs LeakyReLU

    âœ… Experimented with dropout rates

    âœ… Batch sizes: 32, 64, 128

    âœ… Visualized incorrect predictions

    âœ… Used data augmentation on STL-10

ğŸ‘¨â€ğŸ’» Author

Bandlapalli Roshan Babu
M.Tech AI & DS @ Mahindra University
ğŸ“§ broshann14@gmail.com
ğŸ”— LinkedIn
ğŸ”— GitHub
ğŸ“œ License

This project is open-source under the MIT License.
ğŸŒŸ Acknowledgments

    MNIST Dataset

    STL-10 Dataset

    PyTorch & Keras documentation for model reference

ğŸ™Œ Fun Fact

Training vision models from scratch teaches patience and the art of tuning. Deep learning isnâ€™t magicâ€”itâ€™s just great math with smart trial and error! ğŸ§ ğŸ”¥


---

Let me know if you'd like `.md` download support, sample `.py` training code for `mnist_cnn.py`, or a zipped starter project folder with `README`, code, and visuals!

