# ğŸ”¢ Neural Network Function Approximation using TensorFlow

This project demonstrates how to approximate the mathematical function:

\[
y = x_1^2 + x_2^2
\]

using a feedforward neural network in TensorFlow. We experiment with different activation functions (Sigmoid and ReLU) in the hidden layers and analyze their performance on multiple datasets.

---

## ğŸ“Š Objective

Train a neural network to approximate the function:

\[
y = x_1^2 + x_2^2
\]

where \( x_1 \) and \( x_2 \) are scalar inputs. The model is evaluated using two test datasets of different ranges and distributions.

---

## ğŸ“ Project Structure
```
ğŸ“¦ function-approximation-nn
â”œâ”€â”€ generate_data.py # Scripts to generate training and test datasets
â”œâ”€â”€ model_sigmoid.py # Neural network using sigmoid activations
â”œâ”€â”€ model_relu.py # Neural network using ReLU activations
â”œâ”€â”€ utils.py # Plotting and normalization helpers
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ training_surface_sigmoid.png
â”‚ â”œâ”€â”€ test1_surface_sigmoid.png
â”‚ â”œâ”€â”€ test2_surface_sigmoid.png
â”‚ â”œâ”€â”€ training_surface_relu.png
â”‚ â”œâ”€â”€ test1_surface_relu.png
â”‚ â”œâ”€â”€ test2_surface_relu.png
â”œâ”€â”€ README.md

```
---

## ğŸ§ª Dataset Description

### ğŸŸ¢ Training Dataset (483 samples)

- Cartesian product of:
  - \( x_1 \in [-22, -20, ..., 22] \) (step = 2)
  - \( x_2 \in [-10, -9, ..., 10] \) (step = 1)

### ğŸ”µ Test Set 1 (2,500 samples)

- \( x_1 \in [-20, 20] \) (50 evenly spaced values)
- \( x_2 \in [-10, 10] \) (50 evenly spaced values)

### ğŸ”´ Test Set 2 (2,500 samples)

- \( x_1 \in [-50, 50] \) (50 evenly spaced values)
- \( x_2 \in [-30, 30] \) (50 evenly spaced values)

---

## âš™ï¸ Model Architecture

- Input layer: 2 neurons (for \( x_1, x_2 \))
- Hidden layers: Configurable neurons
- Output layer: 1 neuron (linear activation)

Both versions use:
- **MSE Loss Function**
- **Adam Optimizer**
- **Linear output layer**

---

## ğŸ§  Experiments

### âœ… Sigmoid Activation (Hidden Layers)

| Dataset | MSE Loss |
|---------|----------|
| Training | `...` |
| Test1    | `...` |
| Test2    | `...` |

**Training Surface**

![Sigmoid Training Output](results/training_surface_sigmoid.png)

**Test1 Surface**

![Sigmoid Test1 Output](results/test1_surface_sigmoid.png)

**Test2 Surface**

![Sigmoid Test2 Output](results/test2_surface_sigmoid.png)

---

### âœ… ReLU Activation (Hidden Layers)

| Dataset | MSE Loss |
|---------|----------|
| Training | `...` |
| Test1    | `...` |
| Test2    | `...` |



## ğŸ“ˆ Visualizations

All outputs are plotted as 3D surfaces:

- X-axis: \( x_1 \)
- Y-axis: \( x_2 \)
- Z-axis: \( y = f(x_1, x_2) \)

Color maps and surface grids help visualize the function approximation behavior for each activation function.

---

## ğŸš€ How to Run

```bash
# Generate datasets
python generate_data.py
```
# Train with sigmoid
```
python model_sigmoid.py
```
# Train with ReLU
```
python model_relu.py
```
